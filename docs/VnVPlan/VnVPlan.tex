\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}


\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb 24 & 1.0 & Initial Draft\\
\bottomrule
\end{tabularx}

~\\


\newpage

\tableofcontents

\listoftables

%section: list of figures
%\listoffigures

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  A & Assumption\\
  R & Requirement\\
  SRS & Software Requirements Specification\\
  ProtGNN & Prototype-based Graph Neural Network\\
  Re-ProtGNN & Re-implementation of the ProtGNN model\\
  GNN & Graph Neural Network\\
  GIN & Graph Isomorphism Network\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

This document outlines the Verification and Validation (VnV) plan for Re-ProtGNN, a prototype-based interpretable Graph Neural Network. The objective of this plan is to ensure that the system meets the functional and non-functional requirements specified in the Software Requirements Specification (SRS). The document is structured as follows: Section 2 provides general information about Re-ProtGNN, including its objectives and scope. Section 3 details the verification strategies, covering SRS verification, design verification, and implementation verification. Section 4 describes the system tests, including functional and non-functional testing. Finally, Section 5 will cover additional test descriptions as needed.

\section{General Information}

\subsection{Summary}

The software under validation is Re-ProtGNN, a model designed to enhance the interpretability of Graph Neural Networks. It aims to classify graph-structured data while generating prototypes that explain its predictions. The system consists of two main components:

\begin{itemize}
    \item Training Phase: Learns meaningful representations of graphs while optimizing a loss function to improve both classification accuracy and prototype relevance.
    \item Inference Phase: Uses the trained model to classify unseen graphs and generate a set of representative prototypes that provide human-interpretable explanations.
\end{itemize}

Re-ProtGNN is implemented in Python, utilizing PyTorch Geometric for graph learning and Pytest for automated testing.


\subsection{Objectives}

The main goals of this VnV plan are:

\begin{itemize}
    \item Correctness: Ensure that the system correctly processes graph-structured inputs, trains models effectively, and generates meaningful prototype-based explanations.
    \item Accuracy: Assess whether the model attains a minimum classification accuracy of 80\% on the MUTAG dataset and confirm that the selected prototypes align with expected graph representations.
    \item Interpretability: Validate that the extracted prototypes contribute to a clearer understanding of the model’s decision-making process.
\end{itemize}

Out-of-Scope Objectives:
\begin{itemize}
    \item External Library Verification: Core dependencies such as PyTorch and Torch-Geometric are presumed to be reliable and are not explicitly tested in this plan.
    \item Graph Encoder Validation: Graph encoders, including Graph Isomorphism Networks (GINs), are adopted from prior research, and their correctness is assumed without additional validation.
\end{itemize}

\subsection{Challenge Level and Extras}
This is a research project, but extras may be included if time permits.

\subsection{Relevant Documentation}

The Re-ProtGNN project is supported by several key documents that ensure the system is properly designed, implemented, and validated. These documents include:

\begin{itemize}
    \item Problem Statement: This document~\cite{yuanqi2025protgnn} introduces the motivation of Re-ProtGNN and the core problem it aims to solve.
    
    \item Software Requirements Specification (SRS): The SRS~\cite{Yuanqi_ReProtGNN_SRS} defines the functional and non-functional requirements of Re-ProtGNN, and it also outlines the expected behavior of the system.
\end{itemize}

\section{Plan}

This section outlines the Verification and Validation (VnV) plan for Re-ProtGNN. It starts with an introduction to the verification and validation team (Subsection~\ref{sec:vvt}), detailing the members and their roles. Next, it covers the SRS verification plan (Subsection~\ref{sec:srsvp}), followed by the design verification plan (Subsection ~\ref{sec:dvp}). The document then presents the VnV verification plan (Subsection~\ref{sec:vvp}) and the implementation verification plan (Subsection~\ref{sec:ivp}). Finally, it includes automated testing and verification tools (Subsection~\ref{sec:att}) and concludes with the software validation plan (Subsection~\ref{sec:svp}).

\subsection{Verification and Validation Team}
\label{sec:vvt}

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|p{5cm}|}
        \hline
        \textbf{Name} & \textbf{Document} & \textbf{Role} & \textbf{Description} \\ 
        \hline
        Yuanqi Xue & All & Author & Create and manage all required documents, develop the VnV plan, conduct VnV testing, and verify the implementation. \\ 
        \hline
        Dr. Spencer Smith & All & Instructor/ Reviewer & Review all the documents. \\ 
        \hline
        Yinying Huo & All & Domain Expert & Review all the documents. \\ 
        \hline
    \end{tabular}
    \caption{Verification and Validation Team}
\end{table}

\subsection{SRS Verification Plan}
\label{sec:srsvp}
An initial review of the SRS will be conducted by Dr. Spencer Smith and Yinying Huo to ensure its accuracy, completeness, and feasibility. The review will follow a manual inspection process using an SRS Checklist to assess the clarity and alignment of SRS with project objectives.

Reviewers will provide feedback via GitHub issues, and Yuanqi Xue, as the author, will be responsible for addressing revisions.

\subsection{Design Verification Plan}
\label{sec:dvp}
The design verification process, covering the Module Guide (MG) and Module Interface Specification (MIS), will be conducted by Dr. Spencer Smith and domain expert Yinying Huo. Their feedback will be shared through GitHub issues, and Yuanqi Xue will be responsible for making the necessary revisions.

To ensure a structured verification process, Dr. Spencer Smith has created an MG checklist and MIS checklist, both of which will be used to evaluate clarity, consistency, and correctness in the design documents. The verification process will ensure that the system architecture, module interactions, and design choices align with the project objectives.

\subsection{Verification and Validation Plan Verification Plan}
\label{sec:vvp}
The Verification and Validation (VnV) Plan will be reviewed by Dr. Spencer Smith and domain expert Yinying Huo to ensure that the validation methodology aligns with the project’s objectives. Feedback from reviewers will be provided via GitHub issues, where all necessary revisions and updates will be documented.

To maintain consistency and thorough evaluation, a VnV checklist prepared by Dr. Spencer Smith will be used to assess the completeness, correctness, and applicability of the verification and validation process. 


\subsection{Implementation Verification Plan}
\label{sec:ivp}

The implementation of Re-ProtGNN will be verified through a combination of unit tests, system tests, and a final code walkthrough during the class presentation. This ensures that both functional and non-functional requirements are met, as outlined in Section~\ref{sec:system-tests}.

\subsubsection{Static Verification Techniques} 
    \begin{itemize} 
    \item A code walkthrough will be conducted during the final class presentation, where the author, Yuanqi Xue, will present the system architecture, key implementation details, and model behavior.
    
    \item The walkthrough will provide an opportunity for peer review, allowing classmates and the instructor to identify potential issues and suggest improvements. 

\end{itemize}

\subsubsection{Dynamic Testing} 
\begin{itemize} 
    \item The system will undergo unit and system testing, validating both functional and non-functional requirements as specified in Section~\ref{sec:system-tests}. 
    \item Unit tests will focus on key components such as data preprocessing, model training, and prototype generation. 
    \item System tests will evaluate the end-to-end performance of Re-ProtGNN, ensuring a desired classification accuracy and prototype demonstration. 
    \item The testing tools and methodologies are detailed in Section~\ref{subsec: verification-tools}, and test cases are outlined in Section~\ref{sec:system-tests}. 
\end{itemize}


\subsection{Automated Testing and Verification Tools}
\label{sec:att}
\label{subsec: verification-tools}
Re-ProtGNN will use a combination of automated testing and verification tools to ensure code correctness:

Unit Testing:
Pytest will be used for testing individual components, including data processing, loss functions, and inference logic. These tests will help verify that each module functions as expected before integration into the full system.

Continuous Integration (CI):
GitHub Actions will be configured to automate testing after each commit. The workflow will include:
\begin{itemize}
    \item Running unit tests to verify functionality.
    \item Performing data integrity checks.
    \item Validating dependencies to prevent compatibility issues.
\end{itemize}

Regression Testing:
A dedicated test suite will ensure that changes do not negatively impact previously validated functionality. Logs from test runs will be automatically recorded and analyzed to track issues over time, ensuring that updates and modifications do not introduce unexpected errors.



\subsection{Software Validation Plan}
\label{sec:svp}
A separate 20\% test split from the MUTAG dataset will be used to assess the model’s classification effectiveness and the clarity of its prototype-based explanations, with accuracy serving as the main benchmark.

\section{System Tests}
\label{sec:system-tests}
This section outlines the system tests designed to evaluate both functional and non-functional requirements.

\subsection{Tests for Functional Requirements}

This section outlines the tests designed to validate the functional requirements of Re-ProtGNN, ensuring the system behaves as expected under various conditions. The testing areas include input verification, model training correctness, and inference validation, which corresponds to the R1, R2, and R3 of \href{https://github.com/Yuanqi-X/Re-ProtGNN/blob/main/docs/SRS/SRS.pdf}{SRS} ~\cite{Yuanqi_ReProtGNN_SRS}.

\subsubsection{Area of Testing1: Input Verification}

We need to make sure that all input graphs follow the correct format and structure before training begins. This test checks whether the system properly handles the input dataset files and follows the constraints listed in the \href{https://github.com/Yuanqi-X/Re-ProtGNN/blob/main/docs/SRS/SRS.pdf}{SRS} ~\cite{Yuanqi_ReProtGNN_SRS}.
		
\paragraph{Test for Valid Inputs}

\begin{enumerate}


\item{T1: Valid Inputs\\}

Control: Automated Test
					
Initial State: Pending Input
					
Input: The MUTAG dataset files.
					
Output:  If the input is valid, the system should load it successfully. Otherwise, it should return an appropriate error message, as in Table~\ref{tab:graph_validation}.
					
Test Case Derivation: Since Graph Neural Networks (GNNs) rely on structured input, this test makes sure the model doesn't break when given bad data.
					
How test will be performed: An automated script will try loading the MUTAG dataset using the data loader and check whether the system correctly accepts or rejects them.

\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{10pt} % Reduce column spacing
    \begin{tabularx}{\linewidth}{l X c c X} 
        \toprule
        \textbf{ID} & \textbf{Graph Type} & \textbf{Valid?} & \textbf{Error Message} \\
        \midrule
        TI-1 & MUTAG (Adj+Feat) & Yes & NONE \\
        TI-2 & MUTAG (Missing Feats) & No & Missing Node Features \\
        TI-3 & Corrupted Graph & No & Invalid Adjacency Matrix \\
        TI-4 & Non-Graph Input & No & Incorrect Input Type \\
        \bottomrule
    \end{tabularx}
    \caption{Validation results for loading the MUTAG dataset}
    \label{tab:graph_validation}
\end{table}

\end{enumerate}

\subsubsection{Area of Testing2: Model Training Verification}
We need to make sure the training process work. In other words, the model should be able to optimize its loss function and learn properly.

\paragraph{Test for Training Loss Optimization}

\begin{enumerate}
					
\item{T2: Model Training Test\\}

Control: Automatic
					
Initial State: Model is initialized with random weights.
					
Input: The loaded MUTAG dataset.
					
Output: The system should optimize the loss function and display a decreasing loss curve over multiple epochs.

Test Case Derivation: Ensures that the model is learning from the input data instead of failing due to improper gradients or data mismatches.

How test will be performed: Train the model on valid datasets and log the loss over time. A Pytest script will validate loss reduction.

\end{enumerate}

\subsubsection{Area of Testing3: Inference Verification}
This test ensures that the trained model makes accurate predictions and correctly retrieves prototype-based explanations.

\paragraph{Test for Inference}

\begin{enumerate}
					
\item{T3: Inference Test\\}

Control: Automatic
					
Initial State: Model is trained and ready for inference.
					
Input: The testing dataset.
					
Output: The model should output a predicted label for each graph and a set of prototypes.

Test Case Derivation: Ensures that the trained model can be used in the inference phase for classifying inputs and generating a customized numbers of prototypes.

How test will be performed: A Pytest script will run inference on the test dataset, comparing the predicted labels with the ground truth to compute classification accuracy. The system should also generate a set of representative prototypes for the entire dataset. Pytest will verify that the correct number of prototypes is produced.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

This section outlines the tests designed to verify the nonfunctional requirements of Re-ProtGNN, including accuracy, usability, and portability.

\subsubsection{Area of Testing1: Accuracy}
		
\paragraph{Accuracy}

\begin{enumerate}

\item{T4: Accuracy Test\\}

Type: Automated
					
Initial State: A trained Re-ProtGNN model is loaded.
					
Input: The testing dataset.
					
Output: The system should achieve a classification accuracy of at least 80\%.
					
How test will be performed: A Pytest script will run inference on the test dataset and compare the predicted labels with ground truth. The script will calculate the overall classification accuracy and ensure it meets the required threshold ($\geq 80\%$). The accuracy percentage will be logged to evaluate model performance.

\end{enumerate}

\subsubsection{Area of Testing2: Usability}

\paragraph{Usability}

\begin{enumerate}
    \item{T5: Usability Test\\}

    Type: Manual
    					
    Initial State: Trained Re-ProtGNN model is available.
    					
    Input: A set of graphs with learned prototypes.
    					
    Output: The system should visualize prototypes as graph images, making them more interpretable than raw adjacency matrices or node feature representations.
    					
    How test will be performed: A set of sample graphs will be processed through the system, and the generated prototype visualizations will be reviewed for clarity and interpretability. Testers will manually verify whether the images effectively represent key graph structures and provide meaningful insights into model decisions. Feedback will be collected to assess usability improvements.
\end{enumerate}

\subsubsection{Area of Testing3: Portability}

\paragraph{Portability}

\begin{enumerate}
    \item{T6: Portability Test\\}

    Type: Manual
    					
    Initial State: None
    					
    Condition: The users should have PyTorch 1.8.0 and Torch-Geometric 2.0.2 installed.
    					
    Result: The system should successfully run on users' machines, with all functions working as expected.
    					
    How test will be performed: The author, Yuanqi Xue, will install the software on Windows, macOS, and Linux systems, verifying that it runs correctly across all platforms.
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
    & T1 & T2 & T3 & T4 & T5 & T6 \\
  \hline
  R1        & X  &   &   &   &   &   \\ \hline
  R2        &   & X  &   &   &   &   \\ \hline
  R3        &   &   & X  &   & X  &   \\ \hline
  NFR1      &   &   &   & X  &   &   \\ \hline
  NFR2      &   &   &   &   &  X &   \\ \hline
  NFR3      &   &   &   &   &   & X  \\ \hline
  \end{tabular}
  \caption{Traceability Matrix of Test Cases and Requirements}
  \label{Table:trace-test-req}
  \end{table}


\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}
%\bibliography{refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
