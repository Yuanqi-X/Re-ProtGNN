\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}


\begin{document}

\title{System Verification and Validation Plan for Re-ProtGNN} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Feb 24 & 1.0 & Initial Draft\\
\bottomrule
\end{tabularx}

~\\


\newpage

\tableofcontents

\listoftables

%section: list of figures
%\listoffigures

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  A & Assumption\\
  R & Requirement\\
  SRS & Software Requirements Specification\\
  ProtGNN & Prototype-based Graph Neural Network\\
  Re-ProtGNN & Re-implementation of the ProtGNN model\\
  GNN & Graph Neural Network\\
  GIN & Graph Isomorphism Network\\
  \bottomrule
\end{tabular}\\

\newpage

\pagenumbering{arabic}

This document outlines the Verification and Validation (VnV) plan for Re-ProtGNN, a prototype-based interpretable Graph Neural Network. The objective of this plan is to ensure that the system meets the functional and non-functional requirements specified in the Software Requirements Specification (SRS). The document is structured as follows: Section 2 provides general information about Re-ProtGNN, including its objectives and scope. Section 3 details the verification strategies, covering SRS verification, design verification, and implementation verification. Section 4 describes the system tests, including functional and non-functional testing. Finally, Section 5 will cover additional test descriptions as needed.

\section{General Information}

\subsection{Summary}

The software under validation is Re-ProtGNN, a model designed to enhance the interpretability of Graph Neural Networks. It aims to classify graph-structured data while generating prototypes that explain its predictions. The system consists of two main components:

\begin{itemize}
    \item Training Phase: Learns meaningful representations of graphs while optimizing a loss function to improve both classification accuracy and prototype relevance.
    \item Inference Phase: Uses the trained model to classify unseen graphs and generate a set of representative prototypes that provide human-interpretable explanations.
\end{itemize}

Re-ProtGNN is implemented in Python, utilizing PyTorch Geometric for graph learning and Pytest for automated testing.


\subsection{Objectives}

The main goals of this VnV plan are:

\begin{itemize}
    \item Correctness: Ensure that the system correctly processes graph-structured inputs, trains models effectively, and generates meaningful prototype-based explanations.
    \item Accuracy: Assess whether the model attains a minimum classification accuracy of 80\% on the MUTAG dataset and confirm that the selected prototypes align with expected graph representations.
    \item Interpretability: Validate that the extracted prototypes contribute to a clearer understanding of the model’s decision-making process.
\end{itemize}

Out-of-Scope Objectives:
\begin{itemize}
    \item External Library Verification: Core dependencies such as PyTorch and Torch-Geometric are presumed to be reliable and are not explicitly tested in this plan.
    \item Graph Encoder Validation: Graph encoders, including Graph Isomorphism Networks (GINs), are adopted from prior research, and their correctness is assumed without additional validation.
\end{itemize}

\subsection{Challenge Level and Extras}
This is a research project, but extras may be included if time permits.

\subsection{Relevant Documentation}

The Re-ProtGNN project is supported by several key documents that ensure the system is properly designed, implemented, and validated. These documents include:

\begin{itemize}
    \item Problem Statement: This document~\cite{yuanqi2025protgnn} introduces the motivation of Re-ProtGNN and the core problem it aims to solve.
    
    \item Software Requirements Specification (SRS): The SRS~\cite{Yuanqi_ReProtGNN_SRS} defines the functional and non-functional requirements of Re-ProtGNN, and it also outlines the expected behavior of the system.
\end{itemize}

\section{Plan}

This section outlines the Verification and Validation (VnV) plan for Re-ProtGNN. It starts with an introduction to the verification and validation team (Subsection~\ref{sec:vvt}), detailing the members and their roles. Next, it covers the SRS verification plan (Subsection~\ref{sec:srsvp}), followed by the design verification plan (Subsection ~\ref{sec:dvp}). The document then presents the VnV verification plan (Subsection~\ref{sec:vvp}) and the implementation verification plan (Subsection~\ref{sec:ivp}). Finally, it includes automated testing and verification tools (Subsection~\ref{sec:att}) and concludes with the software validation plan (Subsection~\ref{sec:svp}).

\subsection{Verification and Validation Team}
\label{sec:vvt}

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|l|p{5cm}|}
        \hline
        \textbf{Name} & \textbf{Document} & \textbf{Role} & \textbf{Description} \\ 
        \hline
        Yuanqi Xue & All & Author & Create and manage all required documents, develop the VnV plan, conduct VnV testing, and verify the implementation. \\ 
        \hline
        Dr. Spencer Smith & All & Instructor/ Reviewer & Review all the documents. \\ 
        \hline
        Yinying Huo & All & Domain Expert & Review all the documents. \\ 
        \hline
    \end{tabular}
    \caption{Verification and Validation Team}
\end{table}

\subsection{SRS Verification Plan}
\label{sec:srsvp}
An initial review of the SRS will be conducted by Dr. Spencer Smith and Yinying Huo to ensure its accuracy, completeness, and feasibility. The review will follow a manual inspection process using an SRS Checklist to assess the clarity and alignment of SRS with project objectives.

Reviewers will provide feedback via GitHub issues, and Yuanqi Xue, as the author, will be responsible for addressing revisions.

\subsection{Design Verification Plan}
\label{sec:dvp}
The design verification process, covering the Module Guide (MG) and Module Interface Specification (MIS), will be conducted by Dr. Spencer Smith and domain expert Yinying Huo. Their feedback will be shared through GitHub issues, and Yuanqi Xue will be responsible for making the necessary revisions.

To ensure a structured verification process, Dr. Spencer Smith has created an MG checklist and MIS checklist, both of which will be used to evaluate clarity, consistency, and correctness in the design documents. The verification process will ensure that the system architecture, module interactions, and design choices align with the project objectives.

\subsection{Verification and Validation Plan Verification Plan}
\label{sec:vvp}
The Verification and Validation (VnV) Plan will be reviewed by Dr. Spencer Smith and domain expert Yinying Huo to ensure that the validation methodology aligns with the project’s objectives. Feedback from reviewers will be provided via GitHub issues, where all necessary revisions and updates will be documented.

To maintain consistency and thorough evaluation, a VnV checklist prepared by Dr. Spencer Smith will be used to assess the completeness, correctness, and applicability of the verification and validation process. 


\subsection{Implementation Verification Plan}
\label{sec:ivp}

The implementation of Re-ProtGNN will be verified through a combination of unit tests, system tests, and a final code walkthrough during the class presentation. This ensures that both functional and non-functional requirements are met, as outlined in Section~\ref{sec:system-tests}.

\subsubsection{Static Verification Techniques} 
    \begin{itemize} 
    \item A code walkthrough will be conducted during the final class presentation, where the author, Yuanqi Xue, will present the system architecture, key implementation details, and model behavior.
    
    \item The walkthrough will provide an opportunity for peer review, allowing classmates and the instructor to identify potential issues and suggest improvements. 

\end{itemize}

\subsubsection{Dynamic Testing} 
\begin{itemize} 
    \item The system will undergo unit and system testing, validating both functional and non-functional requirements as specified in Section~\ref{sec:system-tests}. 
    \item Unit tests will focus on key components such as data preprocessing, model training, and prototype generation. 
    \item System tests will evaluate the end-to-end performance of Re-ProtGNN, ensuring a desired classification accuracy and prototype demonstration. 
    \item The testing tools and methodologies are detailed in Section~\ref{subsec: verification-tools}, and test cases are outlined in Section~\ref{sec:system-tests}. 
\end{itemize}


\subsection{Automated Testing and Verification Tools}
\label{sec:att}
\label{subsec: verification-tools}
Re-ProtGNN will use a combination of automated testing and verification tools to ensure code correctness:

Unit Testing:
Pytest will be used for testing individual components, including data processing, loss functions, and inference logic. These tests will help verify that each module functions as expected before integration into the full system.

Continuous Integration (CI):
GitHub Actions will be configured to automate testing after each commit. The workflow will include:
\begin{itemize}
    \item Running unit tests to verify functionality.
    \item Performing data integrity checks.
    \item Validating dependencies to prevent compatibility issues.
\end{itemize}

Regression Testing:
A dedicated test suite will ensure that changes do not negatively impact previously validated functionality. Logs from test runs will be automatically recorded and analyzed to track issues over time, ensuring that updates and modifications do not introduce unexpected errors.



\subsection{Software Validation Plan}
\label{sec:svp}
A separate 20\% test split from the MUTAG dataset will be used to assess the model’s classification effectiveness and the clarity of its prototype-based explanations, with accuracy serving as the main benchmark.

\section{System Tests}
\label{sec:system-tests}
This section outlines the system tests designed to evaluate both functional and non-functional requirements.

\subsection{Tests for Functional Requirements}
\label{sub:FR}
This section outlines the tests designed to validate the functional requirements of Re-ProtGNN, ensuring the system behaves as expected under various conditions. The testing areas include input verification, model training correctness, and inference validation, which corresponds to the R1, R2, and R3 of \href{https://github.com/Yuanqi-X/Re-ProtGNN/blob/main/docs/SRS/SRS.pdf}{SRS} ~\cite{Yuanqi_ReProtGNN_SRS}.

\subsubsection{Area of Testing1: Input Verification}

We need to make sure that all input graphs follow the correct format and structure before training begins. This test checks whether the system properly handles the input dataset files and follows the constraints listed in the \href{https://github.com/Yuanqi-X/Re-ProtGNN/blob/main/docs/SRS/SRS.pdf}{SRS} ~\cite{Yuanqi_ReProtGNN_SRS}.
		
\paragraph{Test for Valid Inputs}

\begin{enumerate}


\item{T1: Valid Inputs\\}

Control: Automated Test
					
Initial State: Pending Input
					
Input: The MUTAG dataset files.
					
Output:  If the input is valid, the system should load it successfully. Otherwise, it should return an appropriate error message, as in Table~\ref{tab:graph_validation}.
					
Test Case Derivation: Since Graph Neural Networks (GNNs) rely on structured input, this test makes sure the model doesn't break when given bad data.
					
How test will be performed: An automated script will try loading the MUTAG dataset using the data loader and check whether the system correctly accepts or rejects them.

\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{10pt} % Reduce column spacing
    \begin{tabularx}{\linewidth}{l X c c X} 
        \toprule
        \textbf{ID} & \textbf{Graph Type} & \textbf{Valid?} & \textbf{Error Message} \\
        \midrule
        TI-1 & MUTAG (Adj+Feat) & Yes & NONE \\
        TI-2 & MUTAG (Missing Feats) & No & Missing Node Features \\
        TI-3 & Corrupted Graph & No & Invalid Adjacency Matrix \\
        TI-4 & Non-Graph Input & No & Incorrect Input Type \\
        \bottomrule
    \end{tabularx}
    \caption{Validation results for loading the MUTAG dataset}
    \label{tab:graph_validation}
\end{table}

\end{enumerate}

\subsubsection{Area of Testing2: Model Training Verification}
We need to make sure the training process work. In other words, the model should be able to optimize its loss function and learn properly.

\paragraph{Test for Training Loss Optimization}

\begin{enumerate}
					
\item{T2: Model Training Test\\}

Control: Automatic
					
Initial State: Model is initialized with random weights.
					
Input: The loaded MUTAG dataset.
					
Output: The system should optimize the loss function and display a decreasing loss curve over multiple epochs.

Test Case Derivation: Ensures that the model is learning from the input data instead of failing due to improper gradients or data mismatches.

How test will be performed: Train the model on valid datasets and log the loss over time. A Pytest script will validate loss reduction.

\end{enumerate}

\subsubsection{Area of Testing3: Inference Verification}
This test ensures that the trained model makes accurate predictions and correctly retrieves prototype-based explanations.

\paragraph{Test for Inference}

\begin{enumerate}
					
\item{T3: Inference Test\\}

Control: Automatic
					
Initial State: Model is trained and ready for inference.
					
Input: The testing dataset.
					
Output: The model should output a predicted label for each graph and a set of prototypes.

Test Case Derivation: Ensures that the trained model can be used in the inference phase for classifying inputs and generating a customized numbers of prototypes.

How test will be performed: A Pytest script will run inference on the test dataset, comparing the predicted labels with the ground truth to compute classification accuracy. The system should also generate a set of representative prototypes for the entire dataset. Pytest will verify that the correct number of prototypes is produced.

\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

This section outlines the tests designed to verify the nonfunctional requirements of Re-ProtGNN, including accuracy, usability, and portability.

\subsubsection{Reliability}
		
The reliability of the software is demonstrated through the execution of functional requirement tests, as described in Section~\ref{sub:FR} and Section~\ref{sub:unit FR}. These tests ensure the system behaves as expected under defined conditions.


%\begin{enumerate}

%\item{T4: Accuracy Test\\}

%Type: Automated
					
%Initial State: A trained Re-ProtGNN model is loaded.
					
%Input: The testing dataset.
					
%Output: The system should achieve a classification accuracy of at least 80\%.
					
%How test will be performed: A Pytest script will run inference on the test dataset and compare the predicted labels with ground truth. The script will calculate the overall classification accuracy and ensure it meets the required threshold ($\geq 80\%$). The accuracy percentage will be logged to evaluate model performance.

%\end{enumerate}

\subsubsection{Usability}

The usability of the software is measured mainly by the Usability Survey in Section~\ref{sub:survey}.

%\begin{enumerate}
%    \item{T5: Usability Test\\}

%    Type: Manual
    					
%    Initial State: Trained Re-ProtGNN model is available.
    					
%    Input: A set of graphs with learned prototypes.
    					
%    Output: The system should visualize prototypes as graph images, making them more interpretable than raw adjacency matrices or node feature representations.
    					
%    How test will be performed: A set of sample graphs will be processed through the system, and the generated prototype visualizations will be reviewed for clarity and interpretability. Testers will manually verify whether the images effectively represent key graph structures and provide meaningful insights into model decisions. Feedback will be collected to assess usability improvements.
%\end{enumerate}

%\subsubsection{Area of Testing3: Portability}

%\paragraph{Portability}

%\begin{enumerate}
%    \item{T6: Portability Test\\}

%    Type: Manual
    					
%    Initial State: None
    					
%    Condition: The users should have PyTorch 1.8.0 and Torch-Geometric 2.0.2 installed.
    					
%    Result: The system should successfully run on users' machines, with all functions working as expected.
    					
%    How test will be performed: The author, Yuanqi Xue, will install the software on Windows, macOS, and Linux systems, verifying that it runs correctly across all platforms.
%\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[h!]
  \centering
  \begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
    & T1 & T2 & T3 & T4 & T5 & T6 \\
  \hline
  R1        & X  &   &   &   &   &   \\ \hline
  R2        &   & X  &   &   &   &   \\ \hline
  R3        &   &   & X  &   & X  &   \\ \hline
  NFR1      &   &   &   & X  &   &   \\ \hline
  NFR2      &   &   &   &   &  X &   \\ \hline
  NFR3      &   &   &   &   &   & X  \\ \hline
  \end{tabular}
  \caption{Traceability Matrix of Test Cases and Requirements}
  \label{Table:trace-test-req}
  \end{table}


\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}
\label{sub:unit FR}
\subsubsection{Input Format Module (M3)}

This module corresponds to the data ingestion and preprocessing layer responsible for loading datasets, applying transformations, and preparing batched dataloaders for graph classification. The tests below follow the specifications outlined in the MIS, covering access programs such as load\_dataset, \_get\_dataloader, and include wrapper functions for MUTAG datasets. Each test is derived from either a normal usage pattern or a defined edge case. The goal is to verify expected input-output behavior and robust error handling.

\begin{enumerate}

\item{test-M3-1: Dataset Loading Interface Test\\}
Type: Automatic, Functional \\
Initial State: No data loaded \\
Input: Dataset name (e.g., ``MUTAG''), dataset path, and training configuration \\
Output: Dataset object with correct feature and label dimensions \\
Test Case Derivation: This test ensures that the top-level load\_dataset() interface returns a dataset and associated metadata as required by the training pipeline. \\
How test will be performed: Patch internal loaders, call load\_dataset, and verify returned values and internal calls.

\item{test-M3-2: Random Split Dataloader Construction Test\\}
Type: Automatic, Functional \\
Initial State: Loaded raw dataset \\
Input: PyG list of data objects and split ratios \\
Output: Dictionary of train, eval, and test loaders with expected sizes \\
Test Case Derivation: The module must support random data splitting for cross-validation scenarios. \\
How test will be performed: Construct a dummy dataset and pass it to \_get\_dataloader() with split flags enabled.

\item{test-M3-3: Predefined Split Index Test\\}
Type: Automatic, Functional \\
Initial State: Dataset object with embedded split indices \\
Input: Dataset with supplement['split\_indices'] \\
Output: DataLoader split into subsets based on those indices \\
Test Case Derivation: Required to support externally defined splits (e.g., molecule benchmarks). \\
How test will be performed: Patch the dataset to include custom split labels and verify proper loader construction.

\item{test-M3-4: Dataset Wrapper Invocation Test (MUTAG)\\}
Type: Automatic, Functional \\
Initial State: Dataset not yet initialized \\
Input: Dataset directory and name ``MUTAG'' \\
Output: Loaded and preprocessed PyG dataset object \\
Test Case Derivation: The system should route calls correctly to dataset-specific wrappers. \\
How test will be performed: Patch torch.load, simulate dataset content, and verify the correct class is instantiated.

\item{test-M3-5: Unsupported Dataset Handling Test\\}
Type: Automatic, Functional \\
Initial State: No dataset selected \\
Input: An unrecognized dataset name (e.g., ``FakeDataset'') \\
Output: Raised NotImplementedError \\
Test Case Derivation: Ensures graceful failure and debuggability for unsupported options. \\
How test will be performed: Call \_get\_dataset() with an invalid name and assert raised exception.

\end{enumerate}


\subsubsection{Output Visualization Module (M6)}

This module handles logging, model checkpoint saving, and visual explanation rendering for subgraph-based GNN interpretability. Unit tests have been designed to cover each public access routine defined in the MIS for this module, including standard operations for logging, model saving, and drawing subgraphs across supported datasets. The tests address both expected behavior and edge case handling (e.g., unsupported dataset types). The module’s correctness is validated by asserting the file writes and method calls.

\begin{enumerate}

\item{test-M6-1: Logging Append Test\\}
Type: Automatic, Functional \\
Initial State: Log file empty or exists \\
Input: A string log entry \\
Output: Appended log file with a new line containing the input string \\
Test Case Derivation: Ensures the logging utility appends correctly to a fixed log path. \\
How test will be performed: Patch file I/O and verify the correct write call was made.

\item{test-M6-2: Model Save with Best Checkpoint\\}
Type: Automatic, Functional \\
Initial State: Model in memory \\
Input: Model object, accuracy, epoch, checkpoint directory, is\_best=True \\
Output: Saved model file and best checkpoint copy \\
Test Case Derivation: The system must persist and track the best-performing model. \\
How test will be performed: Patch torch.save and shutil.copy, then verify both were invoked.

\item{test-M6-3: Model Save without Best Checkpoint\\}
Type: Automatic, Functional \\
Initial State: Model in memory \\
Input: Same as above, but with is\_best=False \\
Output: Saved model file only \\
Test Case Derivation: Checkpoint should be saved without updating the best model. \\
How test will be performed: Patch dependencies and confirm only save was called.

\item{test-M6-4: Explanation Dispatcher Test (MUTAG)\\}
Type: Automatic, Functional \\
Initial State: Plot utility initialized with MUTAG context \\
Input: NetworkX graph, node list, feature tensor \\
Output: Call to molecule drawing routine \\
Test Case Derivation: Same as above, adapted for molecular feature mapping. \\
How test will be performed: Use torch input and patch the draw method for assertion.

\item{test-M6-5: Subgraph Drawing Output Test\\}
Type: Automatic, Functional \\
Initial State: Explanation utility instantiated \\
Input: Simple path graph and output path \\
Output: PNG image file written to disk \\
Test Case Derivation: Visualization should execute without error and produce a file. \\
How test will be performed: Use tmp\_path to save figure and verify file existence.

\item{test-M6-6: Dataset Routing Failure Test\\}
Type: Automatic, Functional \\
Initial State: Plot utility initialized with invalid dataset name \\
Input: Any graph and node list \\
Output: Raised NotImplementedError \\
Test Case Derivation: Prevent silent failures when dataset name is unsupported. \\
How test will be performed: Patch plotting to suppress file writes and assert the exception is raised.

\end{enumerate}


\subsubsection{Inference Module (M8)}

This module encapsulates the inference-time behavior of the trained GNN model. It is responsible for computing predictions, evaluating performance on unseen data, and logging results. The primary access routine run\_inference is verified using unit tests that validate its output structure, accuracy computation, loss aggregation, and logging side effects. Both functional and boundary conditions (e.g., single-batch datasets) are tested to ensure stability across usage scenarios defined in the MIS.

\begin{enumerate}

\item{test-M8-1: Inference Execution and Output Test\\}
Type: Automatic, Functional \\
Initial State: Trained model and loss function loaded \\
Input: Evaluation dataloader, model object, and loss function \\
Output: Dictionary with loss and accuracy, prediction logits and probabilities \\
Test Case Derivation: Ensures the function executes end-to-end and returns all expected fields. \\
How test will be performed: Pass a dataloader and model to the inference routine and verify return structure.

\item{test-M8-2: Accuracy Calculation Verification\\}
Type: Automatic, Functional \\
Initial State: Model returns known logits and true labels \\
Input: Model with fixed output and corresponding dataloader \\
Output: Correct accuracy calculation in result dictionary \\
Test Case Derivation: Verifies the correctness of the accuracy logic based on class match count. \\
How test will be performed: Use controlled outputs from a model to assert expected accuracy values.

\item{test-M8-3: Inference Logging Test\\}
Type: Automatic, Functional \\
Initial State: Logging file initialized or exists \\
Input: Same as above \\
Output: Record appended to log file \\
Test Case Derivation: The system is expected to log inference results consistently. \\
How test will be performed: Patch the logging function and verify it is invoked with correctly formatted result.

\item{test-M8-4: Batch Aggregation Consistency Test\\}
Type: Automatic, Functional \\
Initial State: Model returns outputs in multiple batches \\
Input: Multi-batch dataloader and inference model \\
Output: Concatenated arrays for predictions and probabilities \\
Test Case Derivation: All batches should be merged for final output consistency. \\
How test will be performed: Provide two or more batches and check that final arrays match the total batch size.

\end{enumerate}


\subsubsection{Explanation Module (M9)}

The Explanation Module is responsible for identifying and visualizing the most relevant subgraph for a given GNN prediction using a Monte Carlo Tree Search (MCTS)–based strategy. The associated unit tests ensure correctness of explanation scores, rollout behavior, subgraph selection, and visualization triggering. Tests are selected to exercise both the black-box interface and key white-box mechanisms described in the MIS, such as prototype similarity scoring, node expansion, and recursive score propagation.

\begin{enumerate}

\item{test-M9-1: MCTS Node Scoring Test\\}
Type: Automatic, Functional \\
Initial State: Node created with initialized state parameters \\
Input: A coalition and dummy graph \\
Output: Valid computed $Q$ and $U$ scores \\
Test Case Derivation: Verifies the scoring behavior of tree nodes used in exploration phase \\
How test will be performed: Instantiate a node and validate returned $Q$ and $U$ values

\item{test-M9-2: Prototype Similarity Test\\}
Type: Automatic, Functional \\
Initial State: Model in evaluation mode with fixed embedding output \\
Input: Coalition and prototype tensor \\
Output: A scalar similarity score \\
Test Case Derivation: Ensures prototype-sample similarity is correctly measured \\
How test will be performed: Use a dummy GNN to compute the similarity for a given coalition

\item{test-M9-3: MCTS Scoring Utility Test\\}
Type: Automatic, Functional \\
Initial State: Multiple candidate nodes with varying priors \\
Input: List of MCTS nodes and scoring function \\
Output: List of computed scores \\
Test Case Derivation: Verifies score selection logic in exploration step \\
How test will be performed: Run scoring on multiple nodes and check ordering and values

\item{test-M9-4: MCTS Rollout Test\\}
Type: Automatic, Functional \\
Initial State: Root node initialized with full coalition \\
Input: Graph and scoring function \\
Output: Updated node visit counts and scores \\
Test Case Derivation: Validates recursive search and backpropagation logic \\
How test will be performed: Perform a rollout and assert structural updates

\item{test-M9-5: Explanation Function Test\\}
Type: Automatic, Functional \\
Initial State: Dummy GNN and graph sample \\
Input: Graph data, model, and prototype \\
Output: Most relevant subgraph node list, score, and embedding \\
Test Case Derivation: Checks output consistency for the high-level explanation function \\
How test will be performed: Call explanation API and verify output types and values


\item{test-M9-6: MCTS Rollout Backpropagation Test\\}
Type: Automatic, Functional \\
Initial State: Root with one child node with initialized score \\
Input: Score function \\
Output: Updated statistics at root node \\
Test Case Derivation: Verifies that child scores correctly propagate up the tree \\
How test will be performed: Trigger rollout and validate state updates on the parent
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}
Unit testing of nonfunctional requirements is considered out of scope for this project. These requirements will primarily be evaluated through the usability survey in Section~\ref{sub:survey}.

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}
%\bibliography{refs/References}

\newpage

\section{Appendix}

\subsection{Usability Survey Questions?}
\label{sub:survey}
Please rate the following statements on a scale from 1 (Strongly Disagree) to 5 (Strongly Agree):

\begin{enumerate}
    \item The system was easy to learn and use.
    \item The interface is clear and intuitive.
    \item The system responded in a reasonable amount of time.
    \item I felt confident using the system without needing extra help.
    \item Error messages, if any, were clear and helpful.
    \item I am satisfied with my overall experience using the system.
\end{enumerate}

\vspace{1em}

\noindent \textbf{Optional Open-ended Questions:}
\begin{enumerate}
    \item What did you like most about the system?
    \item What improvements would you suggest?
\end{enumerate}


\end{document}
